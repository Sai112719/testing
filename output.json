{
    "paragraphs": [
        "Generated on: 2025-03-26 06:58:47",
        "",
        "sassas\nproc means data=test_data mean sum;\nvar Age Salary;\noutput out=summary_results mean= mean_Age mean_Salary\nsum= sum_Age sum_Salary;\nrun;\n```",
        "pythonpython\nsummary_results = sparkSQL(\nf'''SELECT\nAVG(age) AS mean_age,\nSUM(age) AS sum_age,\nAVG(salary) AS mean_salary,\nSUM(salary) AS sum_salary,\nCOUNT(*) ASfreq,\n0 AStypeFROM\ntest_data '''\n)",
        "setdf('summary_results',summary_results)",
        "```",
        "",
        "python\ninit(globals(), workspace, logBaseDir = f'{workspace}/logs',\nscriptName=scriptName, ENV=\"local\")python\n2025-03-26 06:58:29,882 - INFO -\nðŸ“œ Executing Cell 2:",
        "2025-03-26 06:58:29,883 - INFO - âœ… Cell 2 executed successfully.\n2025-03-26 06:58:29,883 - INFO -\nðŸ“œ Executing Cell 4:",
        "scriptName='testcase1_cms_proc_means_basic'\nworkspace = '/sas2py/app/workspace/common'\nimport sys\nsys.path.insert(1, workspace)\nfrom common_pyspark import *\ndef setv(name, val=\"\"):\npyspark.setv(name, val, global_dict=globals())",
        "def sparkSQL(sql):\nreturn executeSQL(dbConnectType='pyspark',spark=cxt.spark,sqlString=sql)",
        "2025-03-26 06:58:38,251 - INFO - âœ… Cell 4 executed successfully.\n2025-03-26 06:58:38,251 - INFO -\nðŸ“œ Executing Cell 6:",
        "sparkSQL(f''' ''')",
        "test_data_lines = [\n[1, 25, 50000],\n[2, 30, 60000],\n[3, 35, 70000],\n[4, 40, 80000],\n[5, 45, 90000]]\ntest_data = cxt.createDatalinesDF(test_data_lines, ['id',\n'age',\n'salary'], npartitions=5)\nsetdf(f'test_data', test_data)",
        "2025-03-26 06:58:40,358 - INFO - âœ… Cell 6 executed successfully.\n2025-03-26 06:58:40,358 - INFO -\nðŸ“œ Executing Cell 8:",
        "2025-03-26 06:58:40,358 - INFO - âœ… Cell 8 executed successfully.\n2025-03-26 06:58:40,358 - INFO -\nðŸ“œ Executing Cell 10:",
        "summary_results = sparkSQL(\nf'''SELECT\nAVG(age) AS mean_age,\nSUM(age) AS sum_age,\nAVG(salary) AS mean_salary,\nSUM(salary) AS sum_salary,\nCOUNT(*) ASfreq,\n0 AStypeFROM\ntest_data '''\n)",
        "2025-03-26 06:58:40,717 - INFO - âœ… Cell 10 executed successfully.\n2025-03-26 06:58:40,717 - INFO -\nðŸ“œ Executing Cell 12:",
        "2025-03-26 06:58:40,717 - INFO - âœ… Cell 12 executed successfully.\n2025-03-26 06:58:40,717 - INFO -\nðŸ“œ Executing Cell 14:",
        "sparkSQL(f''' ''')",
        "test_data2_lines = [\n[35.0, 175, 70000.0, 350000, 5, '0']]\ntest_data2 = cxt.createDatalinesDF(test_data2_lines, ['mean_age',\n'sum_age',\n'mean_salary',\n'sum_salary',\n'freq',\n'type'], npartitions=5)\nsetdf(f'test_data2', test_data2)",
        "2025-03-26 06:58:41,132 - INFO - âœ… Cell 14 executed successfully.\n2025-03-26 06:58:41,132 - INFO -\nðŸ“œ Executing Cell 15:",
        "df1 = convert_to_pandas(test_data2)\ndf2 = convert_to_pandas(test_data)\ncompared_output = compare_dataframes(df1, df2)\nprint(compared_output)",
        "2025-03-26 06:58:41,237 - INFO - âœ… Cell 15 executed successfully.\n2025-03-26 06:58:41,237 - INFO - ðŸŽ¯ Output ofcompared_output: Comparison failed\n2025-03-26 06:58:41,237 - INFO -\n==================================================================================\nðŸ”¹                              End of Execution                              ðŸ”¹\n```",
        "ðŸ”¹                              End of Execution                              ðŸ”¹\n```python\n==================================================================================",
        "```",
        "",
        "sassas\nproc means data=test1 mean max min maxdec=2;\nclass Department;\nvar Salary;\nrun;\n```",
        "pythonpython\ntest1_means = sparkSQL(\nf'''SELECT\ndepartment,\nAVG(salary) AS mean_salary,\nMAX(salary) AS max_salary,\nMIN(salary) AS min_salary,\nCOUNT(*) ASfreq,\nCASE\nWHEN department IS NOT NULL THEN 1   ELSE 0   END AStypeFROM\ntest1\nGROUP BY\nGROUPING SETS(department) '''\n)",
        "setdf('test1_means',test1_means)",
        "```",
        "",
        "python\ninit(globals(), workspace, logBaseDir = f'{workspace}/logs',\nscriptName=scriptName, ENV=\"local\")python\n2025-03-26 06:58:41,241 - INFO -\nðŸ“œ Executing Cell 2:",
        "2025-03-26 06:58:41,241 - INFO - âœ… Cell 2 executed successfully.\n2025-03-26 06:58:41,241 - INFO -\nðŸ“œ Executing Cell 4:",
        "scriptName='testcase2_proc_mean'\nworkspace = '/sas2py/app/workspace/common'\nimport sys\nsys.path.insert(1, workspace)\nfrom common_pyspark import *\ndef setv(name, val=\"\"):\npyspark.setv(name, val, global_dict=globals())",
        "def sparkSQL(sql):\nreturn executeSQL(dbConnectType='pyspark',spark=cxt.spark,sqlString=sql)",
        "2025-03-26 06:58:41,987 - INFO - âœ… Cell 4 executed successfully.\n2025-03-26 06:58:41,987 - INFO -\nðŸ“œ Executing Cell 6:",
        "sparkSQL(f''' ''')",
        "test1_lines = [\n['HR', 50000],\n['IT', 75000],\n['HR', 54000],\n['IT', 80000],\n['HR', 60000],\n['IT', 72000]]\ntest1 = cxt.createDatalinesDF(test1_lines, ['department',\n'salary'], npartitions=5)\nsetdf(f'test1', test1)",
        "2025-03-26 06:58:42,341 - INFO - âœ… Cell 6 executed successfully.\n2025-03-26 06:58:42,341 - INFO -\nðŸ“œ Executing Cell 8:",
        "2025-03-26 06:58:42,341 - INFO - âœ… Cell 8 executed successfully.\n2025-03-26 06:58:42,342 - INFO -\nðŸ“œ Executing Cell 10:",
        "test1_means = sparkSQL(\nf'''SELECT\ndepartment AS department,\nCOUNT(*) ASfreq,\n1 AStypeFROM\ntest1\nGROUP BY\ndepartment '''\n)",
        "2025-03-26 06:58:42,643 - INFO - âœ… Cell 10 executed successfully.\n2025-03-26 06:58:42,643 - INFO -\nðŸ“œ Executing Cell 12:",
        "2025-03-26 06:58:42,643 - INFO - âœ… Cell 12 executed successfully.\n2025-03-26 06:58:42,643 - INFO -\nðŸ“œ Executing Cell 14:",
        "sparkSQL(f''' ''')",
        "test2_lines = [\n['0', 5, 35, 70000, 175, 350000]]\ntest2 = cxt.createDatalinesDF(test2_lines, ['type',\n'freq',\n'mean_age',\n'mean_salary',\n'sum_age',\n'sum_salary'], npartitions=5)\nsetdf(f'test2', test2)",
        "2025-03-26 06:58:43,022 - INFO - âœ… Cell 14 executed successfully.\n2025-03-26 06:58:43,022 - INFO -\nðŸ“œ Executing Cell 15:",
        "df1 = convert_to_pandas(test2)\ndf2 = convert_to_pandas(test1)\ncompared_output = compare_dataframes(df1, df2)\nprint(compared_output)",
        "2025-03-26 06:58:43,072 - INFO - âœ… Cell 15 executed successfully.\n2025-03-26 06:58:43,072 - INFO - ðŸŽ¯ Output ofcompared_output: Comparison failed\n2025-03-26 06:58:43,072 - INFO -\n==================================================================================\nðŸ”¹                              End of Execution                              ðŸ”¹\n```",
        "ðŸ”¹                              End of Execution                              ðŸ”¹\n```python\n==================================================================================",
        "```",
        "",
        "sassas\nproc means data=test mean min max;\nby Department;\nvar Salary;\nrun;\n```",
        "pythonpython\ntest_means = sparkSQL(\nf'''SELECT\ndepartment,\nAVG(salary) AS mean_salary,\nMIN(salary) AS min_salary,\nMAX(salary) AS max_salary,\nCOUNT(*) ASfreq,\n0 AStypeFROM\ntest2\nGROUP BY\ndepartment '''\n)",
        "setdf('test_means',test_means)",
        "```",
        "",
        "python\ninit(globals(), workspace, logBaseDir = f'{workspace}/logs',\nscriptName=scriptName, ENV=\"local\")python\n2025-03-26 06:58:43,077 - INFO -\nðŸ“œ Executing Cell 2:",
        "2025-03-26 06:58:43,077 - INFO - âœ… Cell 2 executed successfully.\n2025-03-26 06:58:43,077 - INFO -\nðŸ“œ Executing Cell 4:",
        "scriptName='testcase3_proc_means_by_department'\nworkspace = '/sas2py/app/workspace/common'\nimport sys\nsys.path.insert(1, workspace)\nfrom common_pyspark import *\ndef setv(name, val=\"\"):\npyspark.setv(name, val, global_dict=globals())",
        "def sparkSQL(sql):\nreturn executeSQL(dbConnectType='pyspark',spark=cxt.spark,sqlString=sql)",
        "2025-03-26 06:58:43,757 - INFO - âœ… Cell 4 executed successfully.\n2025-03-26 06:58:43,757 - INFO -\nðŸ“œ Executing Cell 6:",
        "sparkSQL(f''' ''')",
        "test_lines = [\n['HR', 50000],\n['IT', 75000],\n['HR', 54000],\n['IT', 80000],\n['HR', 60000],\n['IT', 72000]]\ntest = cxt.createDatalinesDF(test_lines, ['department',\n'salary'], npartitions=5)\nsetdf(f'test', test)",
        "2025-03-26 06:58:44,086 - INFO - âœ… Cell 6 executed successfully.\n2025-03-26 06:58:44,086 - INFO -\nðŸ“œ Executing Cell 8:",
        "2025-03-26 06:58:44,087 - INFO - âœ… Cell 8 executed successfully.\n2025-03-26 06:58:44,087 - INFO -\nðŸ“œ Executing Cell 10:",
        "test_means = sparkSQL(\nf'''SELECT\ndepartment AS department,\nCOUNT(*) ASfreq,\n1 AStypeFROM\ntest\nGROUP BY\ndepartment '''\n)",
        "2025-03-26 06:58:44,181 - INFO - âœ… Cell 10 executed successfully.\n2025-03-26 06:58:44,181 - INFO -\nðŸ“œ Executing Cell 12:",
        "2025-03-26 06:58:44,182 - INFO - âœ… Cell 12 executed successfully.\n2025-03-26 06:58:44,182 - INFO -\nðŸ“œ Executing Cell 14:",
        "sparkSQL(f''' ''')",
        "test2_lines = [\n['HR', 54666.67, 60000, 50000, 3, 1],\n['IT', 75666.67, 80000, 72000, 3, 1]]\ntest2 = cxt.createDatalinesDF(test2_lines, ['department',\n'mean_salary',\n'max_salary',\n'min_salary',\n'freq',\n'type'], npartitions=5)\nsetdf(f'test2', test2)",
        "2025-03-26 06:58:44,508 - INFO - âœ… Cell 14 executed successfully.\n2025-03-26 06:58:44,508 - INFO -\nðŸ“œ Executing Cell 15:",
        "df1 = convert_to_pandas(test2)\ndf2 = convert_to_pandas(test)\ncompared_output = compare_dataframes(df1, df2)\nprint(compared_output)",
        "2025-03-26 06:58:44,559 - INFO - âœ… Cell 15 executed successfully.\n2025-03-26 06:58:44,559 - INFO - ðŸŽ¯ Output ofcompared_output: Outputs are not matched\n2025-03-26 06:58:44,559 - INFO -\n==================================================================================\nðŸ”¹                              End of Execution                              ðŸ”¹\n```",
        "ðŸ”¹                              End of Execution                              ðŸ”¹\n```python\n==================================================================================",
        "```",
        "",
        "sassas\nproc means data=test;\nvar Q1 - Q5;\nrun;\n```",
        "pythonpython\nsummary_results = sparkSQL('''\nSELECT\nAVG(Q1) AS mean_Q1, MIN(Q1) AS min_Q1, MAX(Q1) AS max_Q1, STDDEV(Q1) AS stddev_Q1,\nAVG(Q2) AS mean_Q2, MIN(Q2) AS min_Q2, MAX(Q2) AS max_Q2, STDDEV(Q2) AS stddev_Q2,\nAVG(Q3) AS mean_Q3, MIN(Q3) AS min_Q3, MAX(Q3) AS max_Q3, STDDEV(Q3) AS stddev_Q3,\nAVG(Q4) AS mean_Q4, MIN(Q4) AS min_Q4, MAX(Q4) AS max_Q4, STDDEV(Q4) AS stddev_Q4,\nAVG(Q5) AS mean_Q5, MIN(Q5) AS min_Q5, MAX(Q5) AS max_Q5, STDDEV(Q5) AS stddev_Q5\nFROM test\n''')",
        "setdf('summary_results',summary_results)",
        "```",
        "",
        "python\ninit(globals(), workspace, logBaseDir = f'{workspace}/logs',\nscriptName=scriptName, ENV=\"local\")python\n2025-03-26 06:58:44,562 - INFO -\nðŸ“œ Executing Cell 2:",
        "2025-03-26 06:58:44,562 - INFO - âœ… Cell 2 executed successfully.\n2025-03-26 06:58:44,562 - INFO -\nðŸ“œ Executing Cell 4:",
        "scriptName='testcase4'\nworkspace = '/sas2py/app/workspace/common'\nimport sys\nsys.path.insert(1, workspace)\nfrom common_pyspark import *\ndef setv(name, val=\"\"):\npyspark.setv(name, val, global_dict=globals())",
        "def sparkSQL(sql):\nreturn executeSQL(dbConnectType='pyspark',spark=cxt.spark,sqlString=sql)",
        "2025-03-26 06:58:45,194 - INFO - âœ… Cell 4 executed successfully.\n2025-03-26 06:58:45,194 - INFO -\nðŸ“œ Executing Cell 6:",
        "sparkSQL(f''' ''')",
        "test_lines = [\n[1, 10, 20, 30, 40, 50],\n[2, 15, 25, 35, 45, 55],\n[3, 12, 22, 32, 42, 52],\n[4, 14, 24, 34, 44, 54],\n[5, 18, 28, 38, 48, 58]]\ntest = cxt.createDatalinesDF(test_lines, ['id',\n'q1',\n'q2',\n'q3',\n'q4',\n'q5'], npartitions=5)\nsetdf(f'test', test)",
        "2025-03-26 06:58:45,525 - INFO - âœ… Cell 6 executed successfully.\n2025-03-26 06:58:45,525 - INFO -\nðŸ“œ Executing Cell 8:",
        "2025-03-26 06:58:45,526 - INFO - âœ… Cell 8 executed successfully.\n2025-03-26 06:58:45,526 - INFO -\nðŸ“œ Executing Cell 10:",
        "test_means = sparkSQL(\nf'''SELECT\nCOUNT(*) ASfreq,\n0 AStypeFROM\ntest '''\n)",
        "2025-03-26 06:58:45,615 - INFO - âœ… Cell 10 executed successfully.\n2025-03-26 06:58:45,615 - INFO -\nðŸ“œ Executing Cell 12:",
        "2025-03-26 06:58:45,615 - INFO - âœ… Cell 12 executed successfully.\n2025-03-26 06:58:45,615 - INFO -\nðŸ“œ Executing Cell 14:",
        "sparkSQL(f''' ''')",
        "test2_lines = [\n[13.8, 10, 18, 3.03315017762062, '', '', '', '',\n'', '', '', '', '', '', '', '', '', '', '', ''],\n[23.8, 20, 28, 3.0331501776206204, '', '', '', '',\n'', '', '', '', '', '', '', '', '', '', '', ''],\n[33.8, 30, 38, 3.03315017762062, '', '', '', '',\n'', '', '', '', '', '', '', '', '', '', '', ''],\n[43.8, 40, 48, 3.03315017762062, '', '', '', '',\n'', '', '', '', '', '', '', '', '', '', '', ''],\n[53.8, 50, 58, 3.03315017762062, '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]\ntest2 = cxt.createDatalinesDF(test2_lines, ['mean_q1',\n'min_q1',\n'max_q1',\n'stddev_q1',\n'mean_q2',\n'min_q2',\n'max_q2',\n'stddev_q2',\n'mean_q3',\n'min_q3',\n'max_q3',\n'stddev_q3',\n'mean_q4',\n'min_q4',\n'max_q4',\n'stddev_q4',\n'mean_q5',\n'min_q5',\n'max_q5',\n'stddev_q5'], npartitions=5)\nsetdf(f'test2', test2)",
        "2025-03-26 06:58:45,944 - INFO - âœ… Cell 14 executed successfully.\n2025-03-26 06:58:45,944 - INFO -\nðŸ“œ Executing Cell 15:",
        "df1 = convert_to_pandas(test2)\ndf2 = convert_to_pandas(test)\ncompared_output = compare_dataframes(df1, df2)\nprint(compared_output)",
        "2025-03-26 06:58:45,990 - INFO - âœ… Cell 15 executed successfully.\n2025-03-26 06:58:45,990 - INFO - ðŸŽ¯ Output ofcompared_output: Comparison failed\n2025-03-26 06:58:45,990 - INFO -\n==================================================================================\nðŸ”¹                              End of Execution                              ðŸ”¹\n```",
        "ðŸ”¹                              End of Execution                              ðŸ”¹\n```python\n==================================================================================",
        "```",
        "",
        "sassas\nproc means data=test mean median min max;\nclass Category;\nvar Q1 - Q5;\nrun;\n```",
        "pythonpython\nres =  sparkSQL(f'''\nSELECT\ncategory,\nAVG(q1) AS mean_q1,\nMIN(q1) AS min_q1,\nMAX(q1) AS max_q1,",
        "AVG(q2) AS mean_q2,\nMIN(q2) AS min_q2,\nMAX(q2) AS max_q2,",
        "AVG(q3) AS mean_q3,\nMIN(q3) AS min_q3,\nMAX(q3) AS max_q3,",
        "AVG(q4) AS mean_q4,\nMIN(q4) AS min_q4,\nMAX(q4) AS max_q4,",
        "AVG(q5) AS mean_q5,\nMIN(q5) AS min_q5,\nMAX(q5) AS max_q5,",
        "COUNT(*) ASfreq,",
        "CASE\nWHEN category IS NOT NULL THEN 1\nELSE 0\nEND AStype",
        "FROM test",
        "GROUP BY GROUPING SETS ((category));",
        "''')\nsetdf('res',res)",
        "```",
        "",
        "python\ninit(globals(), workspace, logBaseDir = f'{workspace}/logs',\nscriptName=scriptName, ENV=\"local\")python\n2025-03-26 06:58:45,994 - INFO -\nðŸ“œ Executing Cell 2:",
        "2025-03-26 06:58:45,994 - INFO - âœ… Cell 2 executed successfully.\n2025-03-26 06:58:45,994 - INFO -\nðŸ“œ Executing Cell 4:",
        "scriptName='testcase5'\nworkspace = '/sas2py/app/workspace/common'\nimport sys\nsys.path.insert(1, workspace)\nfrom common_pyspark import *\ndef setv(name, val=\"\"):\npyspark.setv(name, val, global_dict=globals())",
        "def sparkSQL(sql):\nreturn executeSQL(dbConnectType='pyspark',spark=cxt.spark,sqlString=sql)",
        "2025-03-26 06:58:46,621 - INFO - âœ… Cell 4 executed successfully.\n2025-03-26 06:58:46,622 - INFO -\nðŸ“œ Executing Cell 6:",
        "sparkSQL(f''' ''')",
        "test_lines = [\n['A', 10, 20, 30, 40, 50],\n['A', 15, 25, 35, 45, 55],\n['B', 5, 10, 15, 20, 25],\n['B', 12, 22, 32, 42, 52],\n['C', 8, 18, 28, 38, 48],\n['C', 14, 24, 34, 44, 54]]\ntest = cxt.createDatalinesDF(test_lines, ['category',\n'q1',\n'q2',\n'q3',\n'q4',\n'q5'], npartitions=5)\nsetdf(f'test', test)",
        "2025-03-26 06:58:46,957 - INFO - âœ… Cell 6 executed successfully.\n2025-03-26 06:58:46,957 - INFO -\nðŸ“œ Executing Cell 8:",
        "2025-03-26 06:58:46,957 - INFO - âœ… Cell 8 executed successfully.\n2025-03-26 06:58:46,957 - INFO -\nðŸ“œ Executing Cell 10:",
        "test_means = sparkSQL(\nf'''SELECT\ncategory AS category,\nCOUNT(*) ASfreq,\n1 AStypeFROM\ntest\nGROUP BY\ncategory '''\n)",
        "2025-03-26 06:58:47,035 - INFO - âœ… Cell 10 executed successfully.\n2025-03-26 06:58:47,035 - INFO -\nðŸ“œ Executing Cell 12:",
        "2025-03-26 06:58:47,035 - INFO - âœ… Cell 12 executed successfully.\n2025-03-26 06:58:47,035 - INFO -\nðŸ“œ Executing Cell 14:",
        "sparkSQL(f''' ''')",
        "test2_lines = [\n['A', 12.5, 10, 15, 22.5, 20, 25, 32.5, 30,\n35, 42.5, 40, 45, 52.5, 50, 55, 2, 1],\n['B', 8.5, 5, 12, 16.0, 10, 22, 23.5, 15,\n32, 31.0, 20, 42, 38.5, 25, 52, 2, 1],\n['C', 11.0, 8, 14, 21.0, 18, 24, 31.0, 28, 34, 41.0, 38, 44, 51.0, 48, 54, 2, 1]]\ntest2 = cxt.createDatalinesDF(test2_lines, ['category',\n'mean_q1',\n'min_q1',\n'max_q1',\n'mean_q2',\n'min_q2',\n'max_q2',\n'mean_q3',\n'min_q3',\n'max_q3',\n'mean_q4',\n'min_q4',\n'max_q4',\n'mean_q5',\n'min_q5',\n'max_q5',\n'freq',\n'type'], npartitions=5)\nsetdf(f'test2', test2)",
        "2025-03-26 06:58:47,347 - INFO - âœ… Cell 14 executed successfully.\n2025-03-26 06:58:47,347 - INFO -\nðŸ“œ Executing Cell 15:",
        "df1 = convert_to_pandas(test)\ndf2 = convert_to_pandas(test2)\ncompared_output = compare_dataframes(df1, df2)\nprint(compared_output)",
        "2025-03-26 06:58:47,389 - INFO - âœ… Cell 15 executed successfully.\n2025-03-26 06:58:47,389 - INFO - ðŸŽ¯ Output ofcompared_output: Comparison failed\n2025-03-26 06:58:47,389 - INFO -\n==================================================================================\nðŸ”¹                              End of Execution                              ðŸ”¹\n```",
        "ðŸ”¹                              End of Execution                              ðŸ”¹\n```python\n==================================================================================",
        "```"
    ]
}